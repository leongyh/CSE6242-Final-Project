{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DoubleType\n",
    "from pyspark.sql.functions import lit, isnan, size, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STS_RAW_TRAIN_PATH = '../data/sts/training.1600000.processed.noemoticon.csv'\n",
    "STS_RAW_TEST_PATH  = '../data/ststestdata.manual.2009.06.14.csv'\n",
    "\n",
    "STS_PROCESED_TRAIN_PATH = '../data/processed/sts/sts_train'\n",
    "STS_PROCESED_TEST_PATH  = '../data/processed/sts/sts_test'\n",
    "\n",
    "COVID_PROCESSED_PATH = '../data/processed/full-tweets-sanitized/tweets-santized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_processed_schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('label', IntegerType(), True),\n",
    "])\n",
    "\n",
    "covid_processed_schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sts_processed_train = spark.read.csv(STS_PROCESED_TRAIN_PATH + '/' + '*.csv', header=False, schema=sts_processed_schema).withColumn('type', lit('train'))\n",
    "df_sts_processed_test  = spark.read.csv(STS_PROCESED_TEST_PATH + '/' + '*.csv', header=False, schema=sts_processed_schema).withColumn('type', lit('test'))\n",
    "df_covid_processed     = spark.read.csv(COVID_PROCESSED_PATH + '/' + '*.csv', header=False, schema=covid_processed_schema).select('id', 'text', lit(None).alias('label')).withColumn('type', lit('covid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_sts_processed_train.union(df_sts_processed_test).union(df_covid_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up null rows on text column\n",
    "df_all = df_all.filter(~ col(\"text\").isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Pipeline\n",
    "1. Tokenize Words\n",
    "2. Build Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "VOCAB_SIZE = 10000\n",
    "MIN_DF     = 5\n",
    "\n",
    "# Default Stop Words\n",
    "default_stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer          = RegexTokenizer(inputCol=\"text\", outputCol=\"tokenized_text\", pattern=\"\\\\W\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"tokenized_text\", outputCol=\"filtered_text\").setStopWords(default_stop_words)\n",
    "vectorizer         = CountVectorizer(inputCol=\"filtered_text\", outputCol=\"features\", vocabSize=VOCAB_SIZE, minDF=MIN_DF)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, vectorizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_fit = pipeline.fit(df_all)\n",
    "df_all_fit   = pipeline_fit.transform(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-----+--------------------+--------------------+--------------------+\n",
      "|        id|                text|label| type|      tokenized_text|       filtered_text|            features|\n",
      "+----------+--------------------+-----+-----+--------------------+--------------------+--------------------+\n",
      "|1467810369|awww that is a bu...|    0|train|[awww, that, is, ...|[awww, bummer, sh...|(10000,[2,13,105,...|\n",
      "|1467810672|is upset that he ...|    0|train|[is, upset, that,...|[upset, update, f...|(10000,[7,72,147,...|\n",
      "|1467810917|i dived many time...|    0|train|[i, dived, many, ...|[dived, many, tim...|(10000,[6,151,188...|\n",
      "|1467811184|my whole body fee...|    0|train|[my, whole, body,...|[whole, body, fee...|(10000,[5,340,402...|\n",
      "|1467811193|no it is not beha...|    0|train|[no, it, is, not,...|[behaving, m, mad...|(10000,[0,23,607]...|\n",
      "|1467811372|  not the whole crew|    0|train|[not, the, whole,...|       [whole, crew]|(10000,[340,1924]...|\n",
      "|1467811592|          need a hug|    0|train|      [need, a, hug]|         [need, hug]|(10000,[32,973],[...|\n",
      "|1467811594|hey long time no ...|    0|train|[hey, long, time,...|[hey, long, time,...|(10000,[0,10,17,2...|\n",
      "|1467811795|nope they did not...|    0|train|[nope, they, did,...|              [nope]| (10000,[892],[1.0])|\n",
      "|1467812025|        que me muera|    0|train|    [que, me, muera]|        [que, muera]|(10000,[2903],[1.0])|\n",
      "|1467812416|spring break in p...|    0|train|[spring, break, i...|[spring, break, p...|(10000,[282,442,1...|\n",
      "|1467812579|i just repierced ...|    0|train|[i, just, repierc...|   [repierced, ears]|(10000,[2066],[1.0])|\n",
      "|1467812723|i could not bear ...|    0|train|[i, could, not, b...|[bear, watch, tho...|(10000,[108,175,1...|\n",
      "|1467812771|it it counts idk ...|    0|train|[it, it, counts, ...|[counts, idk, eit...|(10000,[88,258,36...|\n",
      "|1467812784|i would have been...|    0|train|[i, would, have, ...|[first, gun, real...|(10000,[22,63,67,...|\n",
      "|1467812799|i wish i got to w...|    0|train|[i, wish, i, got,...|[wish, got, watch...|(10000,[13,36,53,...|\n",
      "|1467812964|hollis' death sce...|    0|train|[hollis, death, s...|[hollis, death, s...|(10000,[108,457,5...|\n",
      "|1467813137| about to file taxes|    0|train|[about, to, file,...|       [file, taxes]|(10000,[3156,6930...|\n",
      "|1467813579|ahh ive always wa...|    0|train|[ahh, ive, always...|[ahh, ive, always...|(10000,[8,23,103,...|\n",
      "|1467813782|oh dear were you ...|    0|train|[oh, dear, were, ...|[oh, dear, drinki...|(10000,[33,543,57...|\n",
      "+----------+--------------------+-----+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all_fit.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Naive Bayes\n",
    "Train the NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all_fit.filter(df_all_fit.type == 'train')\n",
    "df_test  = df_all_fit.filter(df_all_fit.type == 'test')\n",
    "df_covid = df_all_fit.filter(df_all_fit.type == 'covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "model = nb.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|                text|label|type|      tokenized_text|       filtered_text|            features|       rawPrediction|         probability|prediction|\n",
      "+---+--------------------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  3|i loooooooovvvvvv...|    4|test|[i, loooooooovvvv...|[loooooooovvvvvve...|(10000,[42,133,10...|[-34.201567574022...|[0.09307668114179...|       1.0|\n",
      "|  4|reading my kindle...|    4|test|[reading, my, kin...|[reading, kindle,...|(10000,[3,8,248,4...|[-47.781892805679...|[0.04470161962106...|       1.0|\n",
      "|  5|ok first assesmen...|    4|test|[ok, first, asses...|[ok, first, asses...|(10000,[63,110,59...|[-42.262571262719...|[0.25596519931714...|       1.0|\n",
      "|  6|you you will love...|    4|test|[you, you, will, ...|[love, kindle, ve...|(10000,[8,12,14,1...|[-89.456693411755...|[0.57194967837615...|       0.0|\n",
      "|  7|fair enough but i...|    4|test|[fair, enough, bu...|[fair, enough, ki...|(10000,[28,312,51...|[-40.846975577008...|[0.53642866601105...|       0.0|\n",
      "|  8|no it is too big ...|    4|test|[no, it, is, too,...|[big, m, quite, h...|(10000,[0,40,141,...|[-37.324038458850...|[0.18443480450760...|       1.0|\n",
      "|  9|fuck this economy...|    0|test|[fuck, this, econ...|[fuck, economy, h...|(10000,[90,441,12...|[-64.828313720175...|[0.99265187270699...|       0.0|\n",
      "| 10|jquery is my new ...|    4|test|[jquery, is, my, ...|[jquery, new, bes...|(10000,[14,87,176...|[-31.971367720790...|[0.11450398868164...|       1.0|\n",
      "| 11|       loves twitter|    4|test|    [loves, twitter]|    [loves, twitter]|(10000,[45,646],[...|[-15.417271753133...|[0.15749133917318...|       1.0|\n",
      "| 12|how can you not l...|    4|test|[how, can, you, n...|[love, obama, mak...|(10000,[8,222,279...|[-33.723092047225...|[0.13141737666399...|       1.0|\n",
      "| 13|check this video ...|    2|test|[check, this, vid...|[check, video, pr...|(10000,[116,169,2...|[-59.633038030620...|[0.04151957600390...|       1.0|\n",
      "| 14|i firmly believe ...|    0|test|[i, firmly, belie...|[firmly, believe,...|(10000,[27,288,29...|[-56.763576595036...|[0.87250173186471...|       0.0|\n",
      "| 15|house corresponde...|    4|test|[house, correspon...|[house, correspon...|(10000,[13,25,30,...|[-69.228523230135...|[0.32040194160872...|       1.0|\n",
      "| 16|watchin espnjus s...|    4|test|[watchin, espnjus...|[watchin, espnjus...|(10000,[14,341,12...|[-33.931860386057...|[0.25856124952583...|       1.0|\n",
      "| 17|dear nike stop wi...|    0|test|[dear, nike, stop...|[dear, nike, stop...|(10000,[8,218,309...|[-66.019006400734...|[0.90396158482857...|       0.0|\n",
      "| 18|lebron best athle...|    4|test|[lebron, best, at...|[lebron, best, at...|(10000,[4,10,27,8...|[-86.769941520751...|[0.48643998199677...|       1.0|\n",
      "| 19|i was talking to ...|    0|test|[i, was, talking,...|[talking, guy, la...|(10000,[25,30,147...|[-100.78501184159...|[0.93463793655081...|       0.0|\n",
      "| 20|       i love lebron|    4|test|   [i, love, lebron]|      [love, lebron]|(10000,[8,3978],[...|[-16.362633687726...|[0.34587212654917...|       1.0|\n",
      "| 21|lebron is a beast...|    0|test|[lebron, is, a, b...|[lebron, beast, m...|(10000,[0,24,249,...|[-48.246909607945...|[0.77439961815665...|       0.0|\n",
      "| 22|  lebron is the boss|    4|test|[lebron, is, the,...|      [lebron, boss]|(10000,[1555,3978...|[-19.680521561102...|[0.62064740114547...|       0.0|\n",
      "+---+--------------------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling COVID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_LABELED_PATH = '../data/processed/full-tweets-labeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_preds = model.transform(df_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                 id|                text|label| type|      tokenized_text|       filtered_text|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|1246892082888945666|my location at mo...| null|covid|[my, location, at...|[location, modi, ...|(10000,[1,44,207,...|[-166.84942564826...|[0.40145002817456...|       1.0|\n",
      "|1246892725158449152|corona covid figh...| null|covid|[corona, covid, f...|[corona, covid, f...|(10000,[1,44,60,1...|[-125.12106286087...|[0.32846164192647...|       1.0|\n",
      "|1246894604307312640|covid digital pai...| null|covid|[covid, digital, ...|[covid, digital, ...|(10000,[1,44,463,...|[-79.079529527734...|[0.15089496652453...|       1.0|\n",
      "|1246894744174759950|dad how long is c...| null|covid|[dad, how, long, ...|[dad, long, coron...|(10000,[5,11,30,4...|[-123.37082602827...|[0.82659853491940...|       0.0|\n",
      "|1246895626236964873|corona hard seltz...| null|covid|[corona, hard, se...|[corona, hard, se...|(10000,[44,119,18...|[-58.387813335044...|[0.02257098918760...|       1.0|\n",
      "|1246895919536246785|im bored and want...| null|covid|[im, bored, and, ...|[im, bored, want,...|(10000,[6,10,18,2...|[-99.283977618186...|[0.95682669107802...|       0.0|\n",
      "|1246897071942250497|drinking beer and...| null|covid|[drinking, beer, ...|[drinking, beer, ...|(10000,[44,46,516...|[-95.369981562279...|[0.08467205913915...|       1.0|\n",
      "|1246897241505357825|pic of the day co...| null|covid|[pic, of, the, da...|[pic, day, corona...|(10000,[1,2,8,44,...|[-109.41060718536...|[0.01758745323578...|       1.0|\n",
      "|1246897416785321998|where the corona ...| null|covid|[where, the, coro...|[corona, somewher...|(10000,[44,1141,1...|[-31.145225610761...|[0.22666537153405...|       1.0|\n",
      "|1246897425136209923|day of the corona...| null|covid|[day, of, the, co...|[day, corona, dia...|(10000,[2,44,132,...|[-120.76202903592...|[0.03211319925346...|       1.0|\n",
      "|1246898088440193034|mek we stamp out ...| null|covid|[mek, we, stamp, ...|[mek, stamp, bad,...|(10000,[44,56,209...|[-84.169795681423...|[0.87902545087110...|       0.0|\n",
      "|1246898124511014912|lecker local sush...| null|covid|[lecker, local, s...|[lecker, local, s...|(10000,[7,20,44,8...|[-141.63721406360...|[0.40476509594912...|       1.0|\n",
      "|1246898487071047683|seen in meredith ...| null|covid|[seen, in, meredi...|[seen, meredith, ...|(10000,[14,44,73,...|[-98.409537478069...|[0.10510313856028...|       1.0|\n",
      "|1246898498647318531|corona against th...| null|covid|[corona, against,...|[corona, coronavi...|(10000,[34,44,395...|[-78.850598525865...|[0.19556242245952...|       1.0|\n",
      "|1246898647666774018|winkaze connected...| null|covid|[winkaze, connect...|[winkaze, connect...|(10000,[21,44,896...|[-116.44269563155...|[0.06007008066374...|       1.0|\n",
      "|1246899966708027392|party memoirs fro...| null|covid|[party, memoirs, ...|[party, memoirs, ...|(10000,[11,12,118...|[-121.39431945490...|[0.84722676995588...|       0.0|\n",
      "|1246900078243012609|life before coron...| null|covid|[life, before, co...|[life, corona, ju...|(10000,[44,47,74,...|[-165.66567473638...|[0.37376802316972...|       1.0|\n",
      "|1246900278088986624|loudly crying fac...| null|covid|[loudly, crying, ...|[loudly, crying, ...|(10000,[1,21,44,3...|[-276.93822028762...|[0.88866940977916...|       0.0|\n",
      "|1246901306083721216|midnightpost coro...| null|covid|[midnightpost, co...|[midnightpost, co...|(10000,[44,580,37...|[-47.533099552576...|[0.20273988590816...|       1.0|\n",
      "|1246901908280877057|an ideal home by ...| null|covid|[an, ideal, home,...|[ideal, home, hea...|(10000,[1,14,20,4...|[-137.82738492225...|[0.26425465173752...|       1.0|\n",
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "weighted_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "covid_preds_final = covid_preds.withColumn(\"weighted_label\", weighted_prob(\"probability\")).select(\"id\", \"prediction\", \"weighted_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_preds_final.repartition(1).write.csv(COVID_LABELED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
