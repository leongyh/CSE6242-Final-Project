{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DoubleType\n",
    "from pyspark.sql.functions import lit, isnan, size, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "STS_RAW_TRAIN_FILE = '../data/sts/training.1600000.processed.noemoticon.csv'\n",
    "STS_RAW_TEST_FILE  = '../data/sts/testdata.manual.2009.06.14.csv'\n",
    "\n",
    "STS_PROCESED_TRAIN_PATH = '../data/processed/sts/sts_train'\n",
    "STS_PROCESED_TEST_PATH  = '../data/processed/sts/sts_test'\n",
    "\n",
    "COVID_PROCESSED_PATH = '../data/processed/full-tweets-sanitized/tweets-sanitized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_raw_schema = StructType([\n",
    "    StructField('label', IntegerType(), True),\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('query', StringType(), True),\n",
    "    StructField('user', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])\n",
    "\n",
    "sts_processed_schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])\n",
    "\n",
    "covid_processed_schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive file name     - md5sum \n",
    "# Final_preprocessed_sts.csv - ec4e0de0560e2ce9a3c11055b6f41894\n",
    "# Test_data_processed.csv    - ee4e572acdbb6dc129ca397f7d3f37bc\n",
    "# \n",
    "# Recover the labels from the raw data necessary for training and testing\n",
    "# \n",
    "df_sts_processed_train = spark.read.csv(STS_PROCESED_TRAIN_PATH + '/' + '*.csv', header=False, schema=sts_processed_schema).withColumn('type', lit('train'))\n",
    "df_sts_processed_test  = spark.read.csv(STS_PROCESED_TEST_PATH + '/' + '*.csv', header=False, schema=sts_processed_schema).withColumn('type', lit('test'))\n",
    "\n",
    "df_sts_raw_train = spark.read.csv(STS_RAW_TRAIN_FILE, header=False, schema=sts_raw_schema).select('id', 'text', 'label').withColumnRenamed('text', 'raw_text')\n",
    "df_sts_processed_train_with_labels = df_sts_processed_train.join(df_sts_raw_train, on=['id']).select('id', 'text', 'label', 'type')\n",
    "\n",
    "df_sts_raw_test = spark.read.csv(STS_RAW_TEST_FILE, header=False, schema=sts_raw_schema).select('id', 'text', 'label').withColumnRenamed('text', 'raw_text')\n",
    "df_sts_processed_test_with_labels = df_sts_processed_train.join(df_sts_raw_test, on=['id']).select('id', 'text', 'label', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_processed     = spark.read.csv(COVID_PROCESSED_PATH + '/' + '*.csv', header=False, schema=covid_processed_schema).select('id', 'text', lit(None).alias('label')).withColumn('type', lit('covid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_sts_processed_train_with_labels.union(df_sts_processed_test_with_labels).union(df_covid_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up null rows on text column\n",
    "df_all = df_all.filter(~ col(\"text\").isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Pipeline\n",
    "1. Tokenize Words\n",
    "2. Build Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "VOCAB_SIZE = 10000\n",
    "MIN_DF     = 5\n",
    "\n",
    "# Default Stop Words\n",
    "default_stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer          = RegexTokenizer(inputCol=\"text\", outputCol=\"tokenized_text\", pattern=\"\\\\W\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"tokenized_text\", outputCol=\"filtered_text\").setStopWords(default_stop_words)\n",
    "vectorizer         = CountVectorizer(inputCol=\"filtered_text\", outputCol=\"features\", vocabSize=VOCAB_SIZE, minDF=MIN_DF)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, vectorizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_fit = pipeline.fit(df_all)\n",
    "df_all_fit   = pipeline_fit.transform(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-----+--------------------+--------------------+--------------------+\n",
      "|        id|                text|label| type|      tokenized_text|       filtered_text|            features|\n",
      "+----------+--------------------+-----+-----+--------------------+--------------------+--------------------+\n",
      "|1467860144|hate limit letter...|    0|train|[hate, limit, let...|[hate, limit, let...|(10000,[22,78,87,...|\n",
      "|1467862225|website fyi pit w...|    4|train|[website, fyi, pi...|[website, fyi, pi...|(10000,[13,103,15...|\n",
      "|1467889791|call hillsong sai...|    0|train|[call, hillsong, ...|[call, hillsong, ...|(10000,[3,15,24,3...|\n",
      "|1467898027|         thought mac|    4|train|      [thought, mac]|      [thought, mac]|(10000,[138,668],...|\n",
      "|1467904302|www nicki like ha...|    0|train|[www, nicki, like...|[www, nicki, like...|(10000,[5,60,71,1...|\n",
      "|1467928749|tire ddd want sle...|    0|train|[tire, ddd, want,...|[tire, ddd, want,...|(10000,[13,32,35,...|\n",
      "|1467946810|  mine find hard fit|    0|train|[mine, find, hard...|[mine, find, hard...|(10000,[180,356,9...|\n",
      "|1467968979|     april come soon|    0|train| [april, come, soon]| [april, come, soon]|(10000,[25,46,121...|\n",
      "|1467987384|publish new post ...|    4|train|[publish, new, po...|[publish, new, po...|(10000,[10,81,136...|\n",
      "|1468005581|good figure like ...|    4|train|[good, figure, li...|[good, figure, li...|(10000,[3,5,15,51...|\n",
      "|1468010346|school life sleep...|    0|train|[school, life, sl...|[school, life, sl...|(10000,[2,35,38,5...|\n",
      "|1468038360|source shine haha...|    4|train|[source, shine, h...|[source, shine, h...|(10000,[268,295,9...|\n",
      "|1468070706|                file|    4|train|              [file]|              [file]|(10000,[1345],[1.0])|\n",
      "|1468071555|omg particle fusi...|    4|train|[omg, particle, f...|[omg, particle, f...|(10000,[24,124,17...|\n",
      "|1468071701|         wheelbarrow|    4|train|       [wheelbarrow]|       [wheelbarrow]|       (10000,[],[])|\n",
      "|1468088102|   saw got chang hot|    4|train|[saw, got, chang,...|[saw, got, chang,...|(10000,[11,167,20...|\n",
      "|1468108670|sad weird she let...|    0|train|[sad, weird, she,...|[sad, weird, lett...|(10000,[17,56,127...|\n",
      "|1468115212|run great thank w...|    0|train|[run, great, than...|[run, great, than...|(10000,[7,12,29,1...|\n",
      "|1468132343|                cold|    0|train|              [cold]|              [cold]| (10000,[245],[1.0])|\n",
      "|1468132370|           thank man|    4|train|        [thank, man]|        [thank, man]|(10000,[12,107],[...|\n",
      "+----------+--------------------+-----+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all_fit.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Naive Bayes\n",
    "Train the NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all_fit.filter(df_all_fit.type == 'train')\n",
    "df_test  = df_all_fit.filter(df_all_fit.type == 'test')\n",
    "df_covid = df_all_fit.filter(df_all_fit.type == 'covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "model = nb.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+----+--------------+-------------+--------+-------------+-----------+----------+\n",
      "| id|text|label|type|tokenized_text|filtered_text|features|rawPrediction|probability|prediction|\n",
      "+---+----+-----+----+--------------+-------------+--------+-------------+-----------+----------+\n",
      "+---+----+-----+----+--------------+-------------+--------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling COVID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_LABELED_PATH = '../data/processed/full-tweets-labeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_preds = model.transform(df_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                 id|                text|label| type|      tokenized_text|       filtered_text|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|1253972715515469824|rt lysol warning ...| null|covid|[rt, lysol, warni...|[rt, lysol, warni...|(10000,[0,2,13,54...|[-150.41648313321...|[0.28591319484982...|       1.0|\n",
      "|1253972715574157314|americans gonna h...| null|covid|[americans, gonna...|[americans, gonna...|(10000,[113,163,2...|[-31.665983410608...|[0.89051145501911...|       0.0|\n",
      "|1253972715599519745|rt the independen...| null|covid|[rt, the, indepen...|[rt, independence...|(10000,[0,444,479...|[-101.52088510706...|[0.01678199294975...|       1.0|\n",
      "|1253972715645497344|rt coronavirus sh...| null|covid|[rt, coronavirus,...|[rt, coronavirus,...|(10000,[0,2,630],...|[-35.581744008635...|[0.22609411476945...|       1.0|\n",
      "|1253972715935055872|nigerians watchin...| null|covid|[nigerians, watch...|[nigerians, watch...|(10000,[1,83,117,...|[-48.543185721137...|[0.39158182234908...|       1.0|\n",
      "|1253972715985199105|rt a malaysian do...| null|covid|[rt, a, malaysian...|[rt, malaysian, d...|(10000,[0,88,208,...|[-156.91442357626...|[0.16330900461012...|       1.0|\n",
      "|1253972716169900033|hope people dont ...| null|covid|[hope, people, do...|[hope, people, do...|(10000,[6,21,29,5...|[-203.52287884826...|[0.99927656516904...|       0.0|\n",
      "|1253972716559966208|rt this is a hear...| null|covid|[rt, this, is, a,...|[rt, heart, break...|(10000,[0,4,40,98...|[-102.44158474713...|[0.69265962662622...|       0.0|\n",
      "|1253972716639551488|yes we have almos...| null|covid|[yes, we, have, a...|[yes, almost, con...|(10000,[1,9,147,3...|[-55.185663376314...|[0.09843514288526...|       1.0|\n",
      "|1253972716748750850|      great reminder| null|covid|   [great, reminder]|   [great, reminder]|(10000,[46,1326],...|[-18.211158119198...|[0.08045039819210...|       1.0|\n",
      "|1253972716757139456|rt opinion fethul...| null|covid|[rt, opinion, fet...|[rt, opinion, fet...|(10000,[0,2,233,2...|[-119.61042953598...|[0.02487675363337...|       1.0|\n",
      "|1253972716828442624|rt our entire cou...| null|covid|[rt, our, entire,...|[rt, entire, coun...|(10000,[0,96,141,...|[-154.15596841300...|[0.20469747209570...|       1.0|\n",
      "|1253972716840943616|rt it has become ...| null|covid|[rt, it, has, bec...|[rt, become, clea...|(10000,[0,1,22,56...|[-106.79797129489...|[0.12149327554388...|       1.0|\n",
      "|1253972716870348804|rt a digital tech...| null|covid|[rt, a, digital, ...|[rt, digital, tec...|(10000,[0,93,259,...|[-124.98337019018...|[0.08439186366610...|       1.0|\n",
      "|1253972716899594241|rt please help ka...| null|covid|[rt, please, help...|[rt, please, help...|(10000,[0,50,57,1...|[-70.673610621417...|[0.36009501456211...|       1.0|\n",
      "|1253972717113675776|rt delhi people t...| null|covid|[rt, delhi, peopl...|[rt, delhi, peopl...|(10000,[0,1,6,67,...|[-141.94876061053...|[0.29422319565402...|       1.0|\n",
      "|1253972717184913408|rt i did not even...| null|covid|[rt, i, did, not,...|[rt, even, consid...|(10000,[0,44,47,4...|[-54.545877371190...|[0.05685881949368...|       1.0|\n",
      "|1253972717294047232|rt wuhan was the ...| null|covid|[rt, wuhan, was, ...|[rt, wuhan, fenta...|(10000,[0,2,41,40...|[-80.826712829372...|[0.53909330515668...|       0.0|\n",
      "|1253972717335973888|rt dear scumbags ...| null|covid|[rt, dear, scumba...|[rt, dear, scumba...|(10000,[0,5,43,12...|[-107.12102028336...|[0.91348630376350...|       0.0|\n",
      "|1253972717407293440|rt rt despite cor...| null|covid|[rt, rt, despite,...|[rt, rt, despite,...|(10000,[0,7,404,7...|[-75.582714819211...|[0.26872150760308...|       1.0|\n",
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "weighted_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "covid_preds_final = covid_preds.withColumn(\"weighted_label\", weighted_prob(\"probability\")).select(\"id\", \"prediction\", \"weighted_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_preds_final.repartition(1).write.csv(COVID_LABELED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
