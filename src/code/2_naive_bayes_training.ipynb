{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DoubleType\n",
    "from pyspark.sql.functions import lit, isnan, size, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "STS_RAW_TRAIN_FILE = '../data/sts/training.1600000.processed.noemoticon.csv'\n",
    "STS_RAW_TEST_FILE  = '../data/sts/testdata.manual.2009.06.14.csv'\n",
    "\n",
    "STS_PROCESED_TRAIN_PATH = '../data/processed/sts/sts_train'\n",
    "STS_PROCESED_TEST_PATH  = '../data/processed/sts/sts_test'\n",
    "\n",
    "COVID_PROCESSED_PATH = '../data/processed/full-tweets-sanitized/tweets-sanitized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_raw_schema = StructType([\n",
    "    StructField('label', IntegerType(), True),\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('query', StringType(), True),\n",
    "    StructField('user', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])\n",
    "\n",
    "sts_processed_schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])\n",
    "\n",
    "covid_processed_schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive file name     - md5sum \n",
    "# Final_preprocessed_sts.csv - ec4e0de0560e2ce9a3c11055b6f41894\n",
    "# Test_data_processed.csv    - ee4e572acdbb6dc129ca397f7d3f37bc\n",
    "# \n",
    "# Recover the labels from the raw data necessary for training and testing\n",
    "# \n",
    "df_sts_processed_train = spark.read.csv(STS_PROCESED_TRAIN_PATH + '/' + '*.csv', header=False, schema=sts_processed_schema).withColumn('type', lit('train'))\n",
    "df_sts_processed_test  = spark.read.csv(STS_PROCESED_TEST_PATH + '/' + '*.csv', header=False, schema=sts_processed_schema).withColumn('type', lit('test'))\n",
    "\n",
    "df_sts_raw_train = spark.read.csv(STS_RAW_TRAIN_FILE, header=False, schema=sts_raw_schema).select('id', 'text', 'label').withColumnRenamed('text', 'raw_text')\n",
    "df_sts_processed_train_with_labels = df_sts_processed_train.join(df_sts_raw_train, on=['id']).select('id', 'text', 'label', 'type')\n",
    "\n",
    "df_sts_raw_test = spark.read.csv(STS_RAW_TEST_FILE, header=False, schema=sts_raw_schema).select('id', 'text', 'label').withColumnRenamed('text', 'raw_text')\n",
    "df_sts_processed_test_with_labels = df_sts_processed_test.join(df_sts_raw_test, on=['id']).select('id', 'text', 'label', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_processed     = spark.read.csv(COVID_PROCESSED_PATH + '/' + '*.csv', header=False, schema=covid_processed_schema).select('id', 'text', lit(None).alias('label')).withColumn('type', lit('covid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_sts_processed_train_with_labels.union(df_sts_processed_test_with_labels).union(df_covid_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up null rows on text column\n",
    "df_all = df_all.filter(~ col(\"text\").isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Pipeline\n",
    "1. Tokenize Words\n",
    "2. Build Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "VOCAB_SIZE = 10000\n",
    "MIN_DF     = 5\n",
    "\n",
    "# Default Stop Words\n",
    "default_stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer          = RegexTokenizer(inputCol=\"text\", outputCol=\"tokenized_text\", pattern=\"\\\\W\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"tokenized_text\", outputCol=\"filtered_text\").setStopWords(default_stop_words)\n",
    "vectorizer         = CountVectorizer(inputCol=\"filtered_text\", outputCol=\"features\", vocabSize=VOCAB_SIZE, minDF=MIN_DF)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, vectorizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_fit = pipeline.fit(df_all)\n",
    "df_all_fit   = pipeline_fit.transform(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_fit.show()\n",
    "df_all.groupBy('type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Naive Bayes\n",
    "Train the NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all_fit.filter(df_all_fit.type == 'train')\n",
    "df_test  = df_all_fit.filter(df_all_fit.type == 'test')\n",
    "df_covid = df_all_fit.filter(df_all_fit.type == 'covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "model = nb.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+----+--------------+-------------+--------+-------------+-----------+----------+\n",
      "| id|text|label|type|tokenized_text|filtered_text|features|rawPrediction|probability|prediction|\n",
      "+---+----+-----+----+--------------+-------------+--------+-------------+-----------+----------+\n",
      "+---+----+-----+----+--------------+-------------+--------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling COVID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_LABELED_PATH = '../data/processed/full-tweets-labeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_preds = model.transform(df_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                 id|                text|label| type|      tokenized_text|       filtered_text|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|1253972715515469824|rt lysol warning ...| null|covid|[rt, lysol, warni...|[rt, lysol, warni...|(10000,[0,2,13,54...|[-150.41648313321...|[0.28591319484982...|       1.0|\n",
      "|1253972715574157314|americans gonna h...| null|covid|[americans, gonna...|[americans, gonna...|(10000,[113,163,2...|[-31.665983410608...|[0.89051145501911...|       0.0|\n",
      "|1253972715599519745|rt the independen...| null|covid|[rt, the, indepen...|[rt, independence...|(10000,[0,444,479...|[-101.52088510706...|[0.01678199294975...|       1.0|\n",
      "|1253972715645497344|rt coronavirus sh...| null|covid|[rt, coronavirus,...|[rt, coronavirus,...|(10000,[0,2,630],...|[-35.581744008635...|[0.22609411476945...|       1.0|\n",
      "|1253972715935055872|nigerians watchin...| null|covid|[nigerians, watch...|[nigerians, watch...|(10000,[1,83,117,...|[-48.543185721137...|[0.39158182234908...|       1.0|\n",
      "|1253972715985199105|rt a malaysian do...| null|covid|[rt, a, malaysian...|[rt, malaysian, d...|(10000,[0,88,208,...|[-156.91442357626...|[0.16330900461012...|       1.0|\n",
      "|1253972716169900033|hope people dont ...| null|covid|[hope, people, do...|[hope, people, do...|(10000,[6,21,29,5...|[-203.52287884826...|[0.99927656516904...|       0.0|\n",
      "|1253972716559966208|rt this is a hear...| null|covid|[rt, this, is, a,...|[rt, heart, break...|(10000,[0,4,40,98...|[-102.44158474713...|[0.69265962662622...|       0.0|\n",
      "|1253972716639551488|yes we have almos...| null|covid|[yes, we, have, a...|[yes, almost, con...|(10000,[1,9,147,3...|[-55.185663376314...|[0.09843514288526...|       1.0|\n",
      "|1253972716748750850|      great reminder| null|covid|   [great, reminder]|   [great, reminder]|(10000,[46,1326],...|[-18.211158119198...|[0.08045039819210...|       1.0|\n",
      "|1253972716757139456|rt opinion fethul...| null|covid|[rt, opinion, fet...|[rt, opinion, fet...|(10000,[0,2,233,2...|[-119.61042953598...|[0.02487675363337...|       1.0|\n",
      "|1253972716828442624|rt our entire cou...| null|covid|[rt, our, entire,...|[rt, entire, coun...|(10000,[0,96,141,...|[-154.15596841300...|[0.20469747209570...|       1.0|\n",
      "|1253972716840943616|rt it has become ...| null|covid|[rt, it, has, bec...|[rt, become, clea...|(10000,[0,1,22,56...|[-106.79797129489...|[0.12149327554388...|       1.0|\n",
      "|1253972716870348804|rt a digital tech...| null|covid|[rt, a, digital, ...|[rt, digital, tec...|(10000,[0,93,259,...|[-124.98337019018...|[0.08439186366610...|       1.0|\n",
      "|1253972716899594241|rt please help ka...| null|covid|[rt, please, help...|[rt, please, help...|(10000,[0,50,57,1...|[-70.673610621417...|[0.36009501456211...|       1.0|\n",
      "|1253972717113675776|rt delhi people t...| null|covid|[rt, delhi, peopl...|[rt, delhi, peopl...|(10000,[0,1,6,67,...|[-141.94876061053...|[0.29422319565402...|       1.0|\n",
      "|1253972717184913408|rt i did not even...| null|covid|[rt, i, did, not,...|[rt, even, consid...|(10000,[0,44,47,4...|[-54.545877371190...|[0.05685881949368...|       1.0|\n",
      "|1253972717294047232|rt wuhan was the ...| null|covid|[rt, wuhan, was, ...|[rt, wuhan, fenta...|(10000,[0,2,41,40...|[-80.826712829372...|[0.53909330515668...|       0.0|\n",
      "|1253972717335973888|rt dear scumbags ...| null|covid|[rt, dear, scumba...|[rt, dear, scumba...|(10000,[0,5,43,12...|[-107.12102028336...|[0.91348630376350...|       0.0|\n",
      "|1253972717407293440|rt rt despite cor...| null|covid|[rt, rt, despite,...|[rt, rt, despite,...|(10000,[0,7,404,7...|[-75.582714819211...|[0.26872150760308...|       1.0|\n",
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "weighted_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "covid_preds_final = covid_preds.withColumn(\"weighted_label\", weighted_prob(\"probability\")).select(\"id\", \"prediction\", \"weighted_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_preds_final.repartition(1).write.csv(COVID_LABELED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
