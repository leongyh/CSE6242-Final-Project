{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DoubleType\n",
    "from pyspark.sql.functions import lit, isnan, size, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "STS_RAW_TRAIN_FILE = '../data/sts/training.1600000.processed.noemoticon.csv'\n",
    "STS_RAW_TEST_FILE  = '../data/sts/testdata.manual.2009.06.14.csv'\n",
    "\n",
    "STS_PROCESED_TRAIN_PATH = '../data/processed/sts/sts_train'\n",
    "STS_PROCESED_TEST_PATH  = '../data/processed/sts/sts_test'\n",
    "\n",
    "COVID_PROCESSED_PATH = '../data/processed/full-tweets-sanitized/tweets-sanitized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_raw_schema = StructType([\n",
    "    StructField('label', IntegerType(), True),\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('query', StringType(), True),\n",
    "    StructField('user', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])\n",
    "\n",
    "sts_processed_schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])\n",
    "\n",
    "covid_processed_schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive file name     - md5sum \n",
    "# Final_preprocessed_sts.csv - ec4e0de0560e2ce9a3c11055b6f41894\n",
    "# Test_data_processed.csv    - ee4e572acdbb6dc129ca397f7d3f37bc\n",
    "# \n",
    "# Recover the labels from the raw data necessary for training and testing\n",
    "# \n",
    "df_sts_processed_train = spark.read.csv(STS_PROCESED_TRAIN_PATH + '/' + '*.csv', header=False, schema=sts_processed_schema).withColumn('type', lit('train'))\n",
    "df_sts_processed_test  = spark.read.csv(STS_PROCESED_TEST_PATH + '/' + '*.csv', header=False, schema=sts_processed_schema).withColumn('type', lit('test'))\n",
    "\n",
    "df_sts_raw_train = spark.read.csv(STS_RAW_TRAIN_FILE, header=False, schema=sts_raw_schema).select('id', 'text', 'label').withColumnRenamed('text', 'raw_text')\n",
    "df_sts_processed_train_with_labels = df_sts_processed_train.join(df_sts_raw_train, on=['id']).select('id', 'text', 'label', 'type')\n",
    "\n",
    "df_sts_raw_test = spark.read.csv(STS_RAW_TEST_FILE, header=False, schema=sts_raw_schema).select('id', 'text', 'label').withColumnRenamed('text', 'raw_text')\n",
    "df_sts_processed_test_with_labels = df_sts_processed_test.join(df_sts_raw_test, on=['id']).select('id', 'text', 'label', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_processed     = spark.read.csv(COVID_PROCESSED_PATH + '/' + '*.csv', header=False, schema=covid_processed_schema).select('id', 'text', lit(None).alias('label')).withColumn('type', lit('covid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_sts_processed_train_with_labels.union(df_sts_processed_test_with_labels).union(df_covid_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up null rows on text column\n",
    "df_all = df_all.filter(~ col(\"text\").isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Pipeline\n",
    "1. Tokenize Words\n",
    "2. Build Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "VOCAB_SIZE = 10000\n",
    "MIN_DF     = 5\n",
    "\n",
    "# Default Stop Words\n",
    "default_stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer          = RegexTokenizer(inputCol=\"text\", outputCol=\"tokenized_text\", pattern=\"\\\\W\")\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"tokenized_text\", outputCol=\"filtered_text\").setStopWords(default_stop_words)\n",
    "vectorizer         = CountVectorizer(inputCol=\"filtered_text\", outputCol=\"features\", vocabSize=VOCAB_SIZE, minDF=MIN_DF)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_words_remover, vectorizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline_fit = pipeline.fit(df_all)\n",
    "df_all_fit   = pipeline_fit.transform(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-----+--------------------+--------------------+--------------------+\n",
      "|        id|                text|label| type|      tokenized_text|       filtered_text|            features|\n",
      "+----------+--------------------+-----+-----+--------------------+--------------------+--------------------+\n",
      "|1467860144|hate limit letter...|    0|train|[hate, limit, let...|[hate, limit, let...|(10000,[22,78,87,...|\n",
      "|1467862225|website fyi pit w...|    4|train|[website, fyi, pi...|[website, fyi, pi...|(10000,[13,103,15...|\n",
      "|1467889791|call hillsong sai...|    0|train|[call, hillsong, ...|[call, hillsong, ...|(10000,[3,15,24,3...|\n",
      "|1467898027|         thought mac|    4|train|      [thought, mac]|      [thought, mac]|(10000,[138,668],...|\n",
      "|1467904302|www nicki like ha...|    0|train|[www, nicki, like...|[www, nicki, like...|(10000,[5,60,71,1...|\n",
      "|1467928749|tire ddd want sle...|    0|train|[tire, ddd, want,...|[tire, ddd, want,...|(10000,[13,32,35,...|\n",
      "|1467946810|  mine find hard fit|    0|train|[mine, find, hard...|[mine, find, hard...|(10000,[180,356,9...|\n",
      "|1467968979|     april come soon|    0|train| [april, come, soon]| [april, come, soon]|(10000,[25,46,121...|\n",
      "|1467987384|publish new post ...|    4|train|[publish, new, po...|[publish, new, po...|(10000,[10,81,136...|\n",
      "|1468005581|good figure like ...|    4|train|[good, figure, li...|[good, figure, li...|(10000,[3,5,15,51...|\n",
      "|1468010346|school life sleep...|    0|train|[school, life, sl...|[school, life, sl...|(10000,[2,35,38,5...|\n",
      "|1468038360|source shine haha...|    4|train|[source, shine, h...|[source, shine, h...|(10000,[268,295,9...|\n",
      "|1468070706|                file|    4|train|              [file]|              [file]|(10000,[1342],[1.0])|\n",
      "|1468071555|omg particle fusi...|    4|train|[omg, particle, f...|[omg, particle, f...|(10000,[24,124,17...|\n",
      "|1468071701|         wheelbarrow|    4|train|       [wheelbarrow]|       [wheelbarrow]|       (10000,[],[])|\n",
      "|1468088102|   saw got chang hot|    4|train|[saw, got, chang,...|[saw, got, chang,...|(10000,[11,167,20...|\n",
      "|1468108670|sad weird she let...|    0|train|[sad, weird, she,...|[sad, weird, lett...|(10000,[17,56,127...|\n",
      "|1468115212|run great thank w...|    0|train|[run, great, than...|[run, great, than...|(10000,[7,12,29,1...|\n",
      "|1468132343|                cold|    0|train|              [cold]|              [cold]| (10000,[245],[1.0])|\n",
      "|1468132370|           thank man|    4|train|        [thank, man]|        [thank, man]|(10000,[12,107],[...|\n",
      "+----------+--------------------+-----+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-------+\n",
      "| type|  count|\n",
      "+-----+-------+\n",
      "|covid| 227800|\n",
      "|train|1586214|\n",
      "| test|    498|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all_fit.show()\n",
    "df_all.groupBy('type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Naive Bayes\n",
    "Train the NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all_fit.filter(df_all_fit.type == 'train')\n",
    "df_test  = df_all_fit.filter(df_all_fit.type == 'test')\n",
    "df_covid = df_all_fit.filter(df_all_fit.type == 'covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "model = nb.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|                text|label|type|      tokenized_text|       filtered_text|            features|       rawPrediction|         probability|prediction|\n",
      "+---+--------------------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  3|kind cool fantasy...|    4|test|[kind, cool, fant...|[kind, cool, fant...|(10000,[39,124,28...|[-29.738939664474...|[0.08647603750995...|       1.0|\n",
      "|  4|read kind love le...|    4|test|[read, kind, love...|[read, kind, love...|(10000,[3,4,40,28...|[-48.318688106123...|[0.06833971544105...|       1.0|\n",
      "|  5|passes kind fuck ...|    4|test|[passes, kind, fu...|[passes, kind, fu...|(10000,[189,272,2...|[-22.292437079434...|[0.39840010410599...|       1.0|\n",
      "|  6|love kind ive mon...|    4|test|[love, kind, ive,...|[love, kind, ive,...|(10000,[4,10,20,2...|[-57.203072570104...|[0.22562053711939...|       1.0|\n",
      "|  7|fair kind think p...|    4|test|[fair, kind, thin...|[fair, kind, thin...|(10000,[19,288,44...|[-29.176754348762...|[0.38205006458847...|       1.0|\n",
      "|  8|      big happy kind|    4|test|  [big, happy, kind]|  [big, happy, kind]|(10000,[33,129,28...|[-21.294930155986...|[0.16743149383848...|       1.0|\n",
      "|  9|fuck economic hat...|    0|test|[fuck, economic, ...|[fuck, economic, ...|(10000,[78,129,18...|[-64.333526978500...|[0.98491349842904...|       0.0|\n",
      "| 10|query new best fr...|    4|test|[query, new, best...|[query, new, best...|(10000,[10,44,76,...|[-29.378018281754...|[0.17180275385303...|       1.0|\n",
      "| 11|        love twitter|    4|test|     [love, twitter]|     [love, twitter]|(10000,[4,31],[1....|[-11.717990529436...|[0.18286638513574...|       1.0|\n",
      "| 12|love obama make joke|    4|test|[love, obama, mak...|[love, obama, mak...|(10000,[4,86,895,...|[-30.535139188189...|[0.17503576416489...|       1.0|\n",
      "| 13|check video presi...|    2|test|[check, video, pr...|[check, video, pr...|(10000,[101,151,2...|[-66.696021885534...|[0.03492383128257...|       1.0|\n",
      "| 14|firmly believe ob...|    0|test|[firmly, believe,...|[firmly, believe,...|(10000,[13,257,20...|[-87.667566311134...|[0.47825350316006...|       1.0|\n",
      "| 15|house correspond ...|    4|test|[house, correspon...|[house, correspon...|(10000,[11,21,104...|[-104.15265207859...|[0.09010097873422...|       1.0|\n",
      "| 16|watch espn ju see...|    4|test|[watch, espn, ju,...|[watch, espn, ju,...|(10000,[10,23,265...|[-95.356718439615...|[0.07452368401095...|       1.0|\n",
      "| 17|dear nike stop fl...|    0|test|[dear, nike, stop...|[dear, nike, stop...|(10000,[4,148,287...|[-73.031727428004...|[0.83814965651769...|       0.0|\n",
      "| 18|learn best athlet...|    4|test|[learn, best, ath...|[learn, best, ath...|(10000,[6,13,76,2...|[-72.949129044330...|[0.30343756764426...|       1.0|\n",
      "| 19|talk guy night te...|    0|test|[talk, guy, night...|[talk, guy, night...|(10000,[21,78,87,...|[-83.946814826661...|[0.38090794379940...|       1.0|\n",
      "| 20|          love learn|    4|test|       [love, learn]|       [love, learn]|(10000,[4,265],[1...|[-13.569390084130...|[0.20866977668237...|       1.0|\n",
      "| 21|learn beast cheer...|    0|test|[learn, beast, ch...|[learn, beast, ch...|(10000,[128,265,5...|[-43.976441521503...|[0.29750911870569...|       1.0|\n",
      "| 22|          learn boss|    4|test|       [learn, boss]|       [learn, boss]|(10000,[265,1206]...|[-17.038384263820...|[0.44169217715602...|       1.0|\n",
      "+---+--------------------+-----+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.10%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "e = evaluator.evaluate(preds.filter(preds['prediction'] == 0))\n",
    "\n",
    "print(\"Accuracy:\", \"{:.2%}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling COVID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_LABELED_PATH = '../data/processed/full-tweets-labeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_preds = model.transform(df_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                 id|                text|label| type|      tokenized_text|       filtered_text|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|               null|     fully_processed| null|covid|   [fully_processed]|   [fully_processed]|       (10000,[],[])|[-0.6927803369175...|[0.50018345546886...|       0.0|\n",
      "|1246892082888945666|local modi sunday...| null|covid|[local, modi, sun...|[local, modi, sun...|(10000,[43,166,29...|[-150.28050776985...|[0.65409787288106...|       0.0|\n",
      "|1246892725158449152|corona vid fight ...| null|covid|[corona, vid, fig...|[corona, vid, fig...|(10000,[1,43,58,6...|[-157.36454688188...|[0.02343751892074...|       1.0|\n",
      "|1246894604307312640|vid digit paint j...| null|covid|[vid, digit, pain...|[vid, digit, pain...|(10000,[1,38,43,3...|[-166.29017867697...|[0.01484468666539...|       1.0|\n",
      "|1246894744174759950|dad long corona g...| null|covid|[dad, long, coron...|[dad, long, coron...|(10000,[5,8,34,43...|[-125.76163634631...|[0.72395167533730...|       0.0|\n",
      "|1246895626236964873|corona hard shelt...| null|covid|[corona, hard, sh...|[corona, hard, sh...|(10000,[43,98,127...|[-187.37034997055...|[0.85998493095724...|       0.0|\n",
      "|1246895919536246785|bore want fast pa...| null|covid|[bore, want, fast...|[bore, want, fast...|(10000,[6,13,43,1...|[-162.91702030508...|[0.82550036518504...|       0.0|\n",
      "|1246897071942250497|drink beer jeep m...| null|covid|[drink, beer, jee...|[drink, beer, jee...|(10000,[43,45,211...|[-109.72570050981...|[0.14566722831460...|       1.0|\n",
      "|1246897241505357825|pic day corona fi...| null|covid|[pic, day, corona...|[pic, day, corona...|(10000,[0,1,4,33,...|[-200.73759425669...|[0.01693407154225...|       1.0|\n",
      "|1246897416785321998|        corona earth| null|covid|     [corona, earth]|     [corona, earth]|(10000,[43,1088],...|[-21.063086425352...|[0.26450661957627...|       1.0|\n",
      "|1246897425136209923|day corona diary ...| null|covid|[day, corona, dia...|[day, corona, dia...|(10000,[0,43,56,5...|[-160.26571579609...|[0.42624480387818...|       1.0|\n",
      "|1246898088440193034|mek stamp bad min...| null|covid|[mek, stamp, bad,...|[mek, stamp, bad,...|(10000,[43,55,306...|[-98.154307383830...|[0.87007483218935...|       0.0|\n",
      "|1246898124511014912|decker local sush...| null|covid|[decker, local, s...|[decker, local, s...|(10000,[7,9,28,43...|[-241.13421840613...|[0.07188418562299...|       1.0|\n",
      "|1246898487071047683|seen meredith new...| null|covid|[seen, meredith, ...|[seen, meredith, ...|(10000,[10,43,53,...|[-81.612263142411...|[0.54146854585188...|       0.0|\n",
      "|1246898498647318531|corona coronaviru...| null|covid|[corona, coronavi...|[corona, coronavi...|(10000,[9,26,28,4...|[-65.596232035385...|[0.23181599449329...|       1.0|\n",
      "|1246898647666774018|win kate connect ...| null|covid|[win, kate, conne...|[win, kate, conne...|(10000,[6,9,16,43...|[-201.76781749314...|[0.20967576895087...|       1.0|\n",
      "|1246899966708027392|parti memoir pre ...| null|covid|[parti, memoir, p...|[parti, memoir, p...|(10000,[8,24,43,5...|[-107.49786205501...|[0.45765089933111...|       1.0|\n",
      "|1246900078243012609|life corona jug p...| null|covid|[life, corona, ju...|[life, corona, ju...|(10000,[38,43,49,...|[-236.11090829184...|[0.01478251132992...|       1.0|\n",
      "|1246900278088986624|loudly cri face l...| null|covid|[loudly, cri, fac...|[loudly, cri, fac...|(10000,[10,16,43,...|[-353.52109388190...|[0.93561824147363...|       0.0|\n",
      "|1246901306083721216|midnight post cor...| null|covid|[midnight, post, ...|[midnight, post, ...|(10000,[43,48,81,...|[-264.12198328701...|[5.83994569343313...|       1.0|\n",
      "+-------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "weighted_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "covid_preds_final = covid_preds.withColumn(\"weighted_label\", weighted_prob(\"probability\")).select(\"id\", \"prediction\", \"weighted_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_preds_final.repartition(1).write.csv(COVID_LABELED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25175702811244977"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
