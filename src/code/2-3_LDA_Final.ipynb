{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, LongType, DoubleType\n",
    "from pyspark.ml.feature import CountVectorizer , IDF, VectorAssembler\n",
    "from pyspark.mllib.linalg import Vector, Vectors, VectorUDT\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf, lit, when, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('local').config(\"spark.driver.memory\", \"15g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to Covid Tweets sanatized for LDA Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_PROCESSED_PATH = '../data/processed/full-tweets-sanitized'\n",
    "\n",
    "COVID_PROCESSED_FILENAME = 'Tweets_fully_processed_LDA.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('full_text', StringType(), True),\n",
    "])\n",
    "\n",
    "df_covid = spark.read.csv(COVID_PROCESSED_PATH+\"/\" + COVID_PROCESSED_FILENAME, header=True, schema=covid_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up and Tokenize\n",
    "\n",
    "There could be rows where there are no words left after cleaning, therefore we need to impute something in there for Tokenizer to work. \n",
    "\n",
    "Tokenizer will convert a String into a List where each entry is a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_no_null = df_covid.na.fill(\"I am null imputed\")\n",
    "tokenizer = Tokenizer(inputCol=\"full_text\", outputCol=\"words\") \n",
    "tokenized = tokenizer.transform(df_covid_no_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep\n",
    "\n",
    "Use the TF-IDF method to extract features of our tweets. Read more about it here: https://spark.apache.org/docs/1.4.1/mllib-feature-extraction.html\n",
    "\n",
    "After some inital runs, results were not ideal, therefore I decided to remove any words that were 3 characters long or less. In addition, a few other words appeared in topics that appeared to make little sense or appeared too often i.e. \"coronavirus\". The StopWordsRemover function can accept a custom list to remove words from which is how I remove them from our tweets. Then I reprocess cvmodel. \n",
    "\n",
    "Example to see the 20 most common vocab words left from the model: list(cvmodel.vocabulary[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"raw_features\")\n",
    "cvmodel = cv.fit(tokenized)\n",
    "\n",
    "more_then_3_charachters = [word for word in cvmodel.vocabulary if len(word) <= 3] \n",
    "more_then_3_charachters.append('coronavirus')\n",
    "more_then_3_charachters.append('corona')\n",
    "more_then_3_charachters.append('tandem')\n",
    "more_then_3_charachters.append('skin')\n",
    "more_then_3_charachters.append('tone')\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords = more_then_3_charachters)\n",
    "wordsDataFrame = remover.transform(tokenized)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"raw_features\")\n",
    "cvmodel = cv.fit(wordsDataFrame)\n",
    "result_cv = cvmodel.transform(wordsDataFrame)\n",
    "\n",
    "\n",
    "#result_cv = cvmodel.transform(tokenized)\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "k parameter tells you how many topics you want, maxIter is how many times the algorithm will run. Found 4 topics to be the sweetspot where you get reasonable output, can run less iterations but 200 or 300 worked best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=4, maxIter=300)\n",
    "model = lda.fit(result_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.describeTopics()\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Topic words rather than indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_list = [[29, 31, 13, 22, 37, 57, 40, 49, 99, 21],\n",
    "              [4, 54, 3, 7, 5, 27, 69, 118, 0, 1]  ,\n",
    "              [23, 39, 0, 86, 25, 44, 130, 4, 71, 152],\n",
    "              [0, 8, 20, 1, 3, 10, 2, 14, 11, 6] ]\n",
    "results = []\n",
    "for j in topics_list:\n",
    "    sub_results = []\n",
    "    for i in j:\n",
    "        sub_results.append(cvmodel.vocabulary[i])\n",
    "    results.append(sub_results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use our model to get predictions\n",
    "\n",
    "Ideally this would be done on unseen text, but we want to determine for each tweet what topic it best falls under. Will return a column which gives probabilites of it falling under a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = result_tfidf\n",
    "test2 = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out Probabilities\n",
    "\n",
    "The results of the model call above result in a Vector format used by PySparks ml library which isn't suitable for export. Used this UDF function to pull out the probabilies so they can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = udf(ith_, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = test2.select('id',ith(\"topicDistribution\", lit(0)).alias('a'),ith(\"topicDistribution\", lit(1)).alias('b'),ith(\"topicDistribution\", lit(2)).alias('c'),ith(\"topicDistribution\", lit(3)).alias('d'))\n",
    "test3 = test3.withColumn(\"Topic\",when((col(\"a\") >= col(\"b\")) & (col(\"a\") >= col(\"c\")) & (col(\"a\") >= col(\"d\")), \"Topic 1\") \\\n",
    "            .when((col(\"b\") >= col(\"a\")) & (col(\"b\") >= col(\"c\")) & (col(\"b\") >= col(\"d\")), \"Topic 2\") \\\n",
    "            .when((col(\"c\") >= col(\"a\")) & (col(\"c\") >= col(\"b\")) & (col(\"c\") >= col(\"d\")), \"Topic 3\").otherwise(\"Topic 4\"))\n",
    "test3 = test3.withColumn(\"Topic Description\", when(col(\"Topic\") == \"Topic 1\", \"Other\").when(col(\"Topic\") == \"Topic 2\", \"Social Distance\").when(col(\"Topic\") == \"Topic 3\", \"Masks\").otherwise(\"Lockdown\"))\n",
    "test3 = test3.select(\"id\",\"Topic\",\"Topic Description\")\n",
    "#test3.write.csv(\"../data/processed/LDA_topics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
