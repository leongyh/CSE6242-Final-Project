{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "import csv\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier as nbc\n",
    "from nltk.tokenize import word_tokenize # or use some other tokenizer\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"Model training\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(appName)\\\n",
    "    .master(master)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"training.1600000.processed.noemoticon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = spark.read.csv(\"testdata.manual.2009.06.14.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  4|1467822272|Mon Apr 06 22:22:...|NO_QUERY|          ersle|I LOVE @Health4Ua...|\n",
      "|  4|1467822273|Mon Apr 06 22:22:...|NO_QUERY|       becca210|im meeting up wit...|\n",
      "|  4|1467822283|Mon Apr 06 22:22:...|NO_QUERY|      Wingman29|@DaRealSunisaKim ...|\n",
      "|  4|1467822287|Mon Apr 06 22:22:...|NO_QUERY|      katarinka|Being sick can be...|\n",
      "|  4|1467822293|Mon Apr 06 22:22:...|NO_QUERY|    _EmilyYoung|@LovesBrooklyn2 h...|\n",
      "|  4|1467822391|Mon Apr 06 22:22:...|NO_QUERY|  ajarofalmonds|@ProductOfFear Yo...|\n",
      "|  4|1467822447|Mon Apr 06 22:22:...|NO_QUERY|      vmdavinci|@r_keith_hill Tha...|\n",
      "|  4|1467822465|Mon Apr 06 22:22:...|NO_QUERY|  jessicavaliyi|@KeepinUpWKris I ...|\n",
      "|  4|1467822489|Mon Apr 06 22:22:...|NO_QUERY|     emmasaur28|@tommcfly ah, con...|\n",
      "|  4|1467822496|Mon Apr 06 22:22:...|NO_QUERY|  SherylBreuker|@e4VoIP I RESPOND...|\n",
      "|  4|1467822530|Mon Apr 06 22:22:...|NO_QUERY|        claaare|crazy day of scho...|\n",
      "|  4|1467822531|Mon Apr 06 22:22:...|NO_QUERY| Hollywood_Trey|@naughtyhaughty H...|\n",
      "|  4|1467822635|Mon Apr 06 22:22:...|NO_QUERY|     JJLuver756|@nileyjileyluver ...|\n",
      "|  4|1467822729|Mon Apr 06 22:22:...|NO_QUERY|      tophatdog|@soundwav2010 At ...|\n",
      "|  4|1467822796|Mon Apr 06 22:22:...|NO_QUERY|toothfairycyber|@LutheranLucciol ...|\n",
      "|  4|1467822814|Mon Apr 06 22:22:...|NO_QUERY|  misstoriblack|Just added tweeti...|\n",
      "|  4|1467822899|Mon Apr 06 22:22:...|NO_QUERY|      estariray|@michellardi i re...|\n",
      "|  4|1467822924|Mon Apr 06 22:22:...|NO_QUERY|         ddjuli|@nicolerichie: yo...|\n",
      "|  4|1467822936|Mon Apr 06 22:22:...|NO_QUERY|    adamjackson|Catching Up on Em...|\n",
      "|  4|1467822964|Mon Apr 06 22:22:...|NO_QUERY|    lightleaves|Dancing around th...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.filter(df._c0 == 4)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|labels|\n",
      "+--------------------+------+\n",
      "|@switchfoot http:...|     0|\n",
      "|is upset that he ...|     0|\n",
      "|@Kenichan I dived...|     0|\n",
      "|my whole body fee...|     0|\n",
      "|@nationwideclass ...|     0|\n",
      "|@Kwesidei not the...|     0|\n",
      "|         Need a hug |     0|\n",
      "|@LOLTrish hey  lo...|     0|\n",
      "|@Tatiana_K nope t...|     0|\n",
      "|@twittera que me ...|     0|\n",
      "|spring break in p...|     0|\n",
      "|I just re-pierced...|     0|\n",
      "|@caregiving I cou...|     0|\n",
      "|@octolinz16 It it...|     0|\n",
      "|@smarrison i woul...|     0|\n",
      "|@iamjazzyfizzle I...|     0|\n",
      "|Hollis' death sce...|     0|\n",
      "|about to file taxes |     0|\n",
      "|@LettyA ahh ive a...|     0|\n",
      "|@FakerPattyPattz ...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+------+\n",
      "|                text|labels|\n",
      "+--------------------+------+\n",
      "|@stellargirl I lo...|     4|\n",
      "|Reading my kindle...|     4|\n",
      "|Ok, first assesme...|     4|\n",
      "|@kenburbary You'l...|     4|\n",
      "|@mikefish  Fair e...|     4|\n",
      "|@richardebaker no...|     4|\n",
      "|Fuck this economy...|     0|\n",
      "|Jquery is my new ...|     4|\n",
      "|       Loves twitter|     4|\n",
      "|how can you not l...|     4|\n",
      "|Check this video ...|     2|\n",
      "|@Karoli I firmly ...|     0|\n",
      "|House Corresponde...|     4|\n",
      "|Watchin Espn..Jus...|     4|\n",
      "|dear nike, stop w...|     0|\n",
      "|#lebron best athl...|     4|\n",
      "|I was talking to ...|     0|\n",
      "|i love lebron. ht...|     4|\n",
      "|@ludajuice Lebron...|     0|\n",
      "|@Pmillzz lebron I...|     4|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_and_label = df.select(\"_c5\",\"_c0\").withColumnRenamed(\"_c0\",\"labels\").withColumnRenamed(\"_c5\",\"text\")\n",
    "text_and_label.show()\n",
    "\n",
    "text_and_label_test = df_testing.select(\"_c5\",\"_c0\").withColumnRenamed(\"_c0\",\"labels\").withColumnRenamed(\"_c5\",\"text\")\n",
    "text_and_label_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|                text|labels|               words|\n",
      "+--------------------+------+--------------------+\n",
      "|@switchfoot http:...|     0|[@switchfoot, htt...|\n",
      "|is upset that he ...|     0|[is, upset, that,...|\n",
      "|@Kenichan I dived...|     0|[@kenichan, i, di...|\n",
      "|my whole body fee...|     0|[my, whole, body,...|\n",
      "|@nationwideclass ...|     0|[@nationwideclass...|\n",
      "|@Kwesidei not the...|     0|[@kwesidei, not, ...|\n",
      "|         Need a hug |     0|      [need, a, hug]|\n",
      "|@LOLTrish hey  lo...|     0|[@loltrish, hey, ...|\n",
      "|@Tatiana_K nope t...|     0|[@tatiana_k, nope...|\n",
      "|@twittera que me ...|     0|[@twittera, que, ...|\n",
      "|spring break in p...|     0|[spring, break, i...|\n",
      "|I just re-pierced...|     0|[i, just, re-pier...|\n",
      "|@caregiving I cou...|     0|[@caregiving, i, ...|\n",
      "|@octolinz16 It it...|     0|[@octolinz16, it,...|\n",
      "|@smarrison i woul...|     0|[@smarrison, i, w...|\n",
      "|@iamjazzyfizzle I...|     0|[@iamjazzyfizzle,...|\n",
      "|Hollis' death sce...|     0|[hollis', death, ...|\n",
      "|about to file taxes |     0|[about, to, file,...|\n",
      "|@LettyA ahh ive a...|     0|[@lettya, ahh, iv...|\n",
      "|@FakerPattyPattz ...|     0|[@fakerpattypattz...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") \n",
    "tokenized = tokenizer.transform(text_and_label)\n",
    "\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "wordsData = tokenizer.transform(text_and_label)\n",
    "\n",
    "hashingTF = HashingTF().setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\n",
    "\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "\n",
    "# word_array = [tokenized.words for row in mvv_list.collect()]\n",
    "# mvv_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|labels|         rawFeatures|\n",
      "+------+--------------------+\n",
      "|     0|(20,[1,5,6,7,8,10...|\n",
      "|     0|(20,[0,1,4,6,7,9,...|\n",
      "|     0|(20,[0,4,5,7,8,11...|\n",
      "|     0|(20,[1,7,8,10,11,...|\n",
      "|     0|(20,[0,2,4,5,6,11...|\n",
      "|     0|(20,[5,7,17,18],[...|\n",
      "|     0|(20,[7,17,18],[1....|\n",
      "|     0|(20,[1,4,6,7,8,9,...|\n",
      "|     0|(20,[6,8,13,14,17...|\n",
      "|     0|(20,[7,9,11],[3.0...|\n",
      "|     0|(20,[3,4,7,8,11,1...|\n",
      "|     0|(20,[6,7,8,16,17]...|\n",
      "|     0|(20,[0,3,5,8,9,11...|\n",
      "|     0|(20,[1,5,6,7,8,9,...|\n",
      "|     0|(20,[4,5,6,7,10,1...|\n",
      "|     0|(20,[0,2,3,6,8,9,...|\n",
      "|     0|(20,[0,2,3,4,5,6,...|\n",
      "|     0|(20,[8,14,16],[2....|\n",
      "|     0|(20,[0,3,8,9,10,1...|\n",
      "|     0|(20,[0,2,4,8,9,11...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.select(\"labels\", \"rawFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Brute force way \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "pandasDF.columns = [\"feature\", \"label\"]\n",
    "feature = pandasDF.feature\n",
    "labels = pandasDF.label\n",
    "\n",
    "pandasDFtest.columns = [\"feature\", \"label\"]\n",
    "featuretest = pandasDFtest.feature\n",
    "labelstest = pandasDFtest.label\n",
    "\n",
    "# Extract features. \n",
    "featuresets = [(label, feature) for index, (label, feature) in pandasDF.iterrows()]\n",
    "featuresets = featuresets[0::10000]\n",
    "# featuresetstest = [(label, feature) for index, (label, feature) in pandasDFtest.iterrows()]\n",
    "\n",
    "all_words = set(word.lower() for passage in featuresets for word in word_tokenize(passage[0]))\n",
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in featuresets]\n",
    "\n",
    "#  Train a classifier\n",
    "classifier = nbc.train(t)\n",
    "# # Test classifier on \"Neo\"\n",
    "\n",
    "test_sentence = \"This is the best band I've ever heard!\"\n",
    "\n",
    "# test_sent_features = {word: (word in word_tokenize(test_sentence.lower())) for word in all_words}\n",
    "# print(test_sent_features)\n",
    "\n",
    "classifier.classify(test_sent_features)\n",
    "\n",
    "classifier.show_most_informative_features(15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
